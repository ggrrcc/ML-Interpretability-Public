{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90aa334",
   "metadata": {},
   "source": [
    "Extracts trading dates from the input file\n",
    "\n",
    "It does this by looking in merged.dta, then sorting the unique end of month dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6beca775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(path='Inputs/Created/merged.dta'):\n",
    "    try:\n",
    "        df = pd.read_stata(path)\n",
    "    except:\n",
    "        df = pd.read_stata('../' + path)\n",
    "    \n",
    "    # Convert the 'date' column to datetime format\n",
    "    df.loc[:, 'date'] = pd.to_datetime(df.loc[:, 'date'], format=\"%Y-%m-%d\")\n",
    "    # Extract the year and month as a new column\n",
    "    df['year_month'] = df['date'].dt.to_period('M')\n",
    "    # Get the last date for each year-month group\n",
    "    last_dates = df.groupby('year_month')['date'].max()\n",
    "    \n",
    "    # Sort the result\n",
    "    sorted_last_dates = sorted(last_dates)\n",
    "\n",
    "    return sorted_last_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723c570",
   "metadata": {},
   "source": [
    "## Inputs & Constants below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add0e58",
   "metadata": {},
   "source": [
    "### Running signal_info.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71897496",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Getting user adjustments from signal_info.py\n",
    "##### But first has to check if this is being run from MarginalAnalysis, which would cause 'missing_signal' to exist as a variable already; if not then missing_signal must be defined to put the Outputs in the right folder\n",
    "##### WARNING: may need to restart kernel if TradingAlgorithms was run before; currently untested\n",
    "if not('missing_signal' in locals() or 'missing_signal' in globals()):\n",
    "    missing_signal = -1\n",
    "    %run Background_Scripts/signal_info.py\n",
    "    signal_label_dict_og, strategy_info = create_signal_label_dict()\n",
    "    signal_label_dict = signal_label_dict_og.copy()\n",
    "    # What has been defined: paths to various inputs, date splitting constants, signal_label_dict, current_signals, and num_signals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72e51c",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Date/Time Initialization\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "time_date = str(dt.datetime.now().strftime('%Y%m%d%H%M'))\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# In-sample Testing Split Configuration\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "is_test_train_split = .68\n",
    "is_test_split_date = '2012-01-01' # 67% split\n",
    "\n",
    "# Have only one of these for the real deal\n",
    "# test_train_split = .7\n",
    "# split_date = '2012-01-01'\n",
    "test_train_split = .802\n",
    "split_date = '2015-01-01' # 80% split\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# File Paths for Input Data\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "market_data_path = \"Inputs/Downloaded/mkt_rf_daily.csv\"\n",
    "# Real data\n",
    "price_data_path = \"Inputs/Downloaded/price_data_1991-2020.csv\"\n",
    "fund_data_path = \"Inputs/Downloaded/Fundamentals_1991-2020.dta\"\n",
    "merged_data_path = \"Inputs/Created/merged.dta\"\n",
    "\n",
    "# Test data\n",
    "price_data_test_path = \"Inputs/Downloaded/price_data_test.csv\"\n",
    "fund_data_test_path = \"Inputs/Downloaded/Fundamentals_test.dta\"\n",
    "merged_data_test_path = \"Inputs/Created/merged_test.dta\"\n",
    "\n",
    "# Full data\n",
    "price_data_full_path = \"Inputs/Downloaded/price_data_full.csv\"\n",
    "fund_data_full_path = \"Inputs/Downloaded/Fundamentals_full.dta\"\n",
    "merged_data_full_path = \"Inputs/Created/merged_full.dta\"\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Other\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Currently based on days. small_ret = 1, big_ret = 12 if monthly data\n",
    "small_ret = 21\n",
    "big_ret = 252\n",
    "# How many stocks in the dataset are needed to trade\n",
    "min_stocks_available = 1\n",
    "# Create mapping\n",
    "map_to_dec = {-1:'sell', 0:'hold', 1:'buy'} # mapping number to decision\n",
    "map_to_num = {'sell':-1, 'hold':0, 'buy':1} # mapping decision to number\n",
    "# Because accounting data is oftentimes released EOD, we can't use that data to trade on the same day\n",
    "min_accounting_lag = 1\n",
    "# Columns for buys & sells dfs\n",
    "buys_sells_columns = (['permno','quantity'] + list(signal_label_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3e947",
   "metadata": {},
   "source": [
    "### Determine if this should run on test, normal, or full data, and then importing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'is_test' is already defined. If not, ask the user whether it's a test or not\n",
    "if not('is_test' in locals() or 'is_test' in globals()):\n",
    "    test_added = ''\n",
    "    while True:\n",
    "        is_test_input = input('Is this a test? (Enter True or False) ') # User prompt\n",
    "        if is_test_input == \"True\":\n",
    "            is_test  = True\n",
    "            # print(is_test)\n",
    "            break\n",
    "        if is_test_input == \"False\":\n",
    "            is_test = False\n",
    "            # print(is_test)\n",
    "            break\n",
    "        else:\n",
    "            print('Try again.')\n",
    "\n",
    "    full_added = ''\n",
    "    if not is_test:\n",
    "        while True:\n",
    "            is_full_input = input('Is this the full data? (Enter True or False) ') # User prompt\n",
    "            if is_full_input == \"True\":\n",
    "                is_full = True\n",
    "                # print(is_full)\n",
    "                break\n",
    "            if is_full_input == \"False\":\n",
    "                is_full = False\n",
    "                # print(is_full)\n",
    "                break\n",
    "            else:\n",
    "                print('Try again.')\n",
    "    else:\n",
    "        is_full = False\n",
    "        # print(is_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not is_test) and (not is_full):\n",
    "    # print('1', is_test, is_full)\n",
    "    # print(test_added, full_added)\n",
    "    price_data = pd.read_csv(price_data_path)\n",
    "    fund_data = pd.read_stata(fund_data_path)\n",
    "    m_path = merged_data_path\n",
    "elif (not is_test) and is_full:\n",
    "    # print('2', is_test, is_full)\n",
    "    full_added = '_full'\n",
    "    price_data = pd.read_csv(price_data_full_path, iterator=True)\n",
    "    fund_data = pd.read_stata(fund_data_full_path, iterator=True)\n",
    "    m_path = merged_data_full_path\n",
    "else:\n",
    "    test_added = '_test'\n",
    "    price_data = pd.read_csv(price_data_test_path)\n",
    "    fund_data = pd.read_stata(fund_data_test_path)\n",
    "    m_path = merged_data_test_path\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Loading and cleaning merged data\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Run initial_data_processor.ipynb if merged.dta or merged_test.dta is missing\n",
    "try:\n",
    "    # from pandas.io.stata import StataReader\n",
    "    # reader = StataReader(m_path)\n",
    "\n",
    "    merged_data = pd.read_stata(m_path)\n",
    "    # for testing purposes\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "\n",
    "    try:\n",
    "        merged_data.drop('index', axis = 1, inplace=True)\n",
    "    except:\n",
    "        aaaa = 0 # If 'index' column doesn't exist, do nothing\n",
    "except:\n",
    "    %run Background_Scripts/initial_data_processor.ipynb\n",
    "    merged_data = pd.read_stata(m_path)\n",
    "     # for testing purposes\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    try:\n",
    "        merged_data.drop('index', axis = 1, inplace=True)\n",
    "    except:\n",
    "        aaaa = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Date setup\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "merged_data['date_old'] = pd.to_datetime(merged_data['date'])\n",
    "ref_date = merged_data['date_old'].min()\n",
    "merged_data['date'] = (merged_data['date_old'] - ref_date).dt.days\n",
    "\n",
    "# Create a sorted list of unique dates from the 'merged_data'\n",
    "udates = sorted(merged_data.loc[:,'date'].unique())\n",
    "# Split the merged data into training and test sets based on the split date (or test split date if it's a test run)\n",
    "if type(split_date) == type('amogus'):\n",
    "    print(split_date)\n",
    "    split_date = dt.datetime.strptime(split_date, '%Y-%m-%d')\n",
    "    print(split_date)\n",
    "else:\n",
    "    print(split_date)\n",
    "split_date_numeric = (split_date - ref_date).days\n",
    "if not is_test:\n",
    "    merged_data_train = merged_data[merged_data['date'] < split_date_numeric]\n",
    "    merged_data_test = merged_data[merged_data['date'] >= split_date_numeric]\n",
    "else:\n",
    "    is_test_split_date = dt.datetime.strptime(is_test_split_date, '%Y-%m-%d')\n",
    "    is_test_split_date_numeric = (is_test_split_date - ref_date).days\n",
    "    merged_data_train = merged_data[merged_data['date'] < is_test_split_date_numeric]\n",
    "    merged_data_test = merged_data[merged_data['date'] >= is_test_split_date_numeric]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58960d0",
   "metadata": {},
   "source": [
    "### Post import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Create a dictionary for fast access to merged data by date\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Create a dictionary mapping each date to a list of the corresponding 'permno' rows (stocks) and their associated signals\n",
    "columns_to_keep = list(merged_data_test.columns)\n",
    "columns_to_keep.remove('date')\n",
    "columns_to_keep.remove('permno')\n",
    "grouped = merged_data_test.groupby('date').apply(lambda x: x.set_index('permno')[columns_to_keep].apply(lambda row: row.tolist(), axis=1).to_dict()).reset_index(name='items')\n",
    "# Convert to dictionary\n",
    "merged_data_dict = dict(zip(grouped['date'], grouped['items']))\n",
    "\n",
    "### Fitting original models- the 'all' column is used for creating models that use all the data (either stocks or shares)\n",
    "stocks = sorted(merged_data['permno'].unique())\n",
    "num_stocks = len(stocks)\n",
    "stocks_og = stocks.copy()\n",
    "stocks = np.append(stocks, 'all_stocks')\n",
    "signal_labels = list(signal_label_dict.keys())\n",
    "signal_labels_og = signal_labels.copy()\n",
    "signal_labels = np.append(signal_labels, 'all_signals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Setup for Google Colab (currently not finished)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# End of Month Trading Dates\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# %run Background_Scripts/EOM_Trading_Dates.ipynb\n",
    "if not is_test:\n",
    "    EOM_dates = extract_dates(m_path)[:-1]\n",
    "    old_trading_dates = EOM_dates[round(test_train_split*len(EOM_dates)-1):]\n",
    "    old_trading_dates = pd.to_datetime(old_trading_dates)\n",
    "    trading_dates = (old_trading_dates - ref_date).days\n",
    "else:\n",
    "    EOM_dates = extract_dates(m_path)[:-1]\n",
    "    old_trading_dates = EOM_dates[round(is_test_train_split*len(EOM_dates)-1):]\n",
    "    old_trading_dates = pd.to_datetime(old_trading_dates)\n",
    "    trading_dates = (old_trading_dates - ref_date).days\n",
    "\n",
    "next_day_map = {}\n",
    "for i in range(len(trading_dates) - 1):\n",
    "    next_day_map[trading_dates[i]] = trading_dates[i + 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
