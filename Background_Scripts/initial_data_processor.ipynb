{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2164dc",
   "metadata": {},
   "source": [
    "This file takes in the input files, merges them, and cleans up the data while adding in buy or sell labels to help the RF models detect what's a good trade and what isn't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0df75f",
   "metadata": {
    "id": "5d0df75f"
   },
   "source": [
    "Todo: Comments (ChatGPT!)\n",
    "\n",
    "Currently uses 7 stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576fb05",
   "metadata": {},
   "source": [
    "# Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff59560",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ff59560",
    "outputId": "e9eb1f98-9df1-4234-8a49-a26a40b5d58e"
   },
   "outputs": [],
   "source": [
    "### Adjust inputs if in Google Colab\n",
    "# Process: Upload this notebook and any needed files to a Google Drive Folder named \"Project_10\" in the same folder structure.\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if not IN_COLAB:\n",
    "    if ('full_added' in locals()) or ('full_added' in globals()):\n",
    "        price_path = \"Inputs/Downloaded/price_data_1991-2020\" + test_added + full_added + \".csv\"\n",
    "        fund_path = \"Inputs/Downloaded/Fundamentals_1991-2020\" + test_added + full_added + \".dta\"\n",
    "        conversion_path = 'Inputs/Downloaded/gvkey_permno_conversion.dta'\n",
    "    else:\n",
    "        test_added = ''\n",
    "        full_added = '_full'\n",
    "        price_path = \"../Inputs/Downloaded/price_data_full\" + test_added + full_added + \".csv\"\n",
    "        fund_path = \"../Inputs/Downloaded/Fundamentals_full\" + test_added + full_added + \".dta\"\n",
    "        conversion_path = '../Inputs/Downloaded/gvkey_permno_conversion.dta'\n",
    "\n",
    "else:\n",
    "    # A few changes done for Google Colab\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "\n",
    "    # # my_path = \"\"\n",
    "    # gdrive_path = \"/content/drive\" + \"/My Drive\" + \"/Project_10/\" # THIS is your GDrive path\n",
    "    # gpath_scripts = gdrive_path + \"Background_Scripts/\"\n",
    "    # gpath_output = gdrive_path + \"Outputs/\"\n",
    "    # gpath_inputs = gdrive_path + \"Inputs/\"\n",
    "\n",
    "    # price_data = gpath_inputs + \"price_data_1991\" + full_added +  test_added + \".csv\"\n",
    "    # fund_data = gpath_inputs + \"Fundamentals_1991\" + full_added + test_added + \".dta\"\n",
    "    # conversion = gpath_inputs + \"gvkey_permno_conversion.dta\"\n",
    "    # merge = gpath_inputs + \"merged\" + full_added + test_added + \".dta\"\n",
    "\n",
    "    # port = gpath_scripts + \"portfolio_db.ipynb\"\n",
    "    # backtest_ex = gpath_scripts + \"backtest_executor.ipynb\"\n",
    "    # backtest_stat = gpath_scripts + \"backtest_statistician.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataframe imports\n",
    "try:\n",
    "    price_df = price_data.copy()\n",
    "    signal_df = fund_data.copy()\n",
    "    gvkey_permno_conversion_df = pd.read_stata(conversion_path).drop(['index', 'fyearq', 'fqtr'], axis = 1)\n",
    "except:\n",
    "    price_df = pd.read_csv(price_path)\n",
    "    signal_df = pd.read_stata(fund_path)\n",
    "    gvkey_permno_conversion_df = pd.read_stata(conversion_path).drop(['index', 'fyearq', 'fqtr'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd72c3a",
   "metadata": {
    "id": "6cd72c3a"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# Constants #\n",
    "#############\n",
    "# Number of days between quarterly earnings announcement and when we can use data\n",
    "# We can use data of up to 4 quarters ago, reported in the last 2 (we always use the most recent data)\n",
    "# Accounting lag is for the time frame of the data, which we won't know until it's reported so we can't use it, which complicates our merging\n",
    "# Report lag is for when the data is reported\n",
    "min_accounting_lag = 1\n",
    "max_accounting_lag = 361\n",
    "min_report_date_lag = 1\n",
    "max_report_date_lag = 181\n",
    "min_lag_dt = np.timedelta64(min_accounting_lag,'D')\n",
    "max_lag_dt = np.timedelta64(max_accounting_lag,'D')\n",
    "min_date_lag_val = 1000000000*60*60*24\n",
    "max_acc_lag_val = 361*min_date_lag_val\n",
    "max_report_date_lag_val = 181*min_date_lag_val\n",
    "\n",
    "# # Define thresholds for classification\n",
    "default_buy_threshold = 0.02\n",
    "default_sell_threshold = -0.02\n",
    "\n",
    "# Currently based on days. small_ret = 1, big_ret = 12 if monthly data\n",
    "small_ret = 21 # 1 month's worth of trading days\n",
    "big_ret = 252 # 1 year's worth of trading days\n",
    "\n",
    "# Minimum share price to open a new position\n",
    "min_share_price = 1.0\n",
    "\n",
    "################\n",
    "# Data cleanup #\n",
    "################\n",
    "# Cols needed to cut down the columns later down the line\n",
    "date_identifying_cols = ['permno', 'int_datadate', 'int_rdq']\n",
    "price_id_cols = ['permno', 'int_date']\n",
    "\n",
    "# Column renames, edits, merge dfs, drop important nulls\n",
    "price_df = price_df.rename(columns = {'PERMNO':'permno'})\n",
    "price_df['RET'] = pd.to_numeric(price_df['RET'], errors='coerce')\n",
    "price_df = price_df[price_df['RET'].notna()]\n",
    "price_df.loc[:,'date'] = pd.to_datetime(price_df.loc[:,'date'], format =\"%Y-%m-%d\")\n",
    "print('merging signal and gvkey')\n",
    "comb_df = signal_df.merge(gvkey_permno_conversion_df, on=['gvkey','datadate'])\n",
    "comb_df.drop('gvkey', axis = 1, inplace = True)\n",
    "comb_df['permno'] = comb_df['permno'].astype(np.int64)\n",
    "\n",
    "del gvkey_permno_conversion_df\n",
    "\n",
    "# This gets rid of datapoints where the stock doesn't have a reporting date for accounting info\n",
    "comb_df = comb_df[pd.notnull(comb_df['rdq'])] # Should probably do this for other datapoints\n",
    "\n",
    "#####################\n",
    "# Setup for merging #\n",
    "#####################\n",
    "# Make it easier to check dates by changing to numbers\n",
    "price_df['int_date'] = price_df['date'].apply(lambda x: x.value)\n",
    "comb_df['int_datadate'] = comb_df['datadate'].apply(lambda x: x.value)\n",
    "comb_df['int_rdq'] = comb_df['rdq'].apply(lambda x: x.value)\n",
    "\n",
    "# Some constants used later\n",
    "unique_permnos = price_df['permno'].unique()\n",
    "# good_indices = []\n",
    "# all_s = pd.Series()\n",
    "\n",
    "price_df = price_df.sort_values(by=['permno', 'int_date'])\n",
    "comb_df = comb_df.sort_values(by=['permno', 'int_rdq', 'int_datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51fe830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_price_df(price_df):\n",
    "\n",
    "    permno_to_date_dict = {}\n",
    "    # total_time0 = 0\n",
    "    # total_time1 = 0\n",
    "    # total_time2 = 0\n",
    "    # total_time3 = 0\n",
    "    # total_time4 = 0\n",
    "    # t0=time.time()\n",
    "\n",
    "    grouped = price_df.groupby('permno')\n",
    "\n",
    "    for p in unique_permnos:\n",
    "        this_permno_dict = {}\n",
    "        # t1 = time.time()\n",
    "        curr_price_df = grouped.get_group(p)\n",
    "        current_trading_days = curr_price_df['int_date'].unique()\n",
    "\n",
    "        curr_signal_df = comb_df[comb_df['permno'] == p]\n",
    "        if curr_price_df.empty or curr_signal_df.empty:\n",
    "            continue\n",
    "        # t2 = time.time()\n",
    "        # total_time0 += (t2-t1)\n",
    "        \n",
    "        int_rdq_list = list(curr_signal_df.loc[:,'int_rdq'])\n",
    "        int_datadate_list = list(curr_signal_df.loc[:,'int_datadate'])\n",
    "        \n",
    "        signal_pointer = 0\n",
    "        for date in current_trading_days:\n",
    "            if signal_pointer >= len(curr_signal_df):\n",
    "                break\n",
    "            # t11 = time.time()\n",
    "            rdq_date = int_rdq_list[signal_pointer]\n",
    "            # t111 = time.time()\n",
    "            # total_time1 += (t111-t11)\n",
    "            if date <= rdq_date:\n",
    "                continue\n",
    "            doPrint = True\n",
    "\n",
    "            # t22 = time.time()\n",
    "            curr_condition = (date>rdq_date) and ((date-rdq_date)<max_report_date_lag_val) and ((date-int_datadate_list[signal_pointer])<max_acc_lag_val)\n",
    "            # t222 = time.time()\n",
    "            # total_time2 += (t222-t22)\n",
    "            if signal_pointer + 1 >= len(curr_signal_df):\n",
    "                next_condition = False\n",
    "            else:\n",
    "                # t33 = time.time()\n",
    "                next_rdq_date = int_rdq_list[signal_pointer+1]\n",
    "                # t333 = time.time()\n",
    "                # total_time3 += (t333-t33)\n",
    "\n",
    "                # t44 = time.time()\n",
    "                next_condition = (date>next_rdq_date) and ((date-next_rdq_date)<max_report_date_lag_val) and ((date-int_datadate_list[signal_pointer+1])<max_acc_lag_val)\n",
    "                # t444 = time.time()\n",
    "                # total_time4 += (t444-t44)\n",
    "            while not curr_condition or (curr_condition and next_condition):\n",
    "                if signal_pointer+1 >= len(curr_signal_df):\n",
    "                    doPrint = False\n",
    "                    break\n",
    "                signal_pointer += 1\n",
    "                curr_condition = (date>rdq_date) and (date-rdq_date<max_report_date_lag_val) and (date-int_datadate_list[signal_pointer]<max_acc_lag_val)\n",
    "                if signal_pointer + 1 >= len(curr_signal_df):\n",
    "                    next_condition = False\n",
    "                else:\n",
    "                    next_rdq_date = int_rdq_list[signal_pointer+1]\n",
    "                    next_condition = (date>next_rdq_date) and ((date-next_rdq_date)<max_report_date_lag_val) and ((date-int_datadate_list[signal_pointer+1])<max_acc_lag_val)\n",
    "            try:\n",
    "                if not doPrint:\n",
    "                    continue\n",
    "                this_permno_dict[date] = rdq_date\n",
    "                # print('added')\n",
    "            except:\n",
    "                a = 0\n",
    "\n",
    "        if this_permno_dict != {}:\n",
    "            permno_to_date_dict[p] = this_permno_dict\n",
    "    # print(total_time0)\n",
    "    # print(total_time1)\n",
    "    # print(total_time2)\n",
    "    # print(total_time3)\n",
    "    # print(total_time4)\n",
    "    # print(time.time()-t0)\n",
    "\n",
    "    price_df['int_rdq'] = price_df.apply(\n",
    "        lambda row: permno_to_date_dict.get(row['permno'], {}).get(row['int_date'], None),\n",
    "        axis=1\n",
    "    )\n",
    "    gc.collect()\n",
    "    return price_df\n",
    "\n",
    "price_df = new_price_df(price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "print('p', price_df[['permno', 'int_rdq']].head())\n",
    "print('col', comb_df.columns)\n",
    "print('c', comb_df[['permno', 'int_rdq']].head())\n",
    "print('u1', price_df['int_rdq'].unique())\n",
    "print('u2', comb_df['int_rdq'].unique())\n",
    "merged_df = pd.merge(price_df, comb_df, on=['permno', 'int_rdq'], how='left')\n",
    "del price_df\n",
    "del comb_df\n",
    "new_m = merged_df[merged_df['int_rdq'].notna()]\n",
    "del merged_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1bb48",
   "metadata": {},
   "source": [
    "## Below signals are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %run Background_Scripts/signal_info.py\n",
    "except:\n",
    "    %run ../Background_Scripts/signal_info.py\n",
    "\n",
    "gc.collect()\n",
    "new_m = add_signals_to_df(new_m, small_ret, big_ret, default_buy_threshold, default_sell_threshold)\n",
    "new_m['permno'] = new_m['permno'].astype(str)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d9acaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_m.to_csv(\"../Inputs/Created/merged_full.csv\")\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55236296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file:\n",
    "# new_m['permno'] = new_m['permno'].astype(str)\n",
    "\n",
    "if IN_COLAB:\n",
    "    new_m.to_stata(merge, convert_dates={'date': 'tc', 'datadate': 'tc', 'rdq': 'tc'})\n",
    "else:\n",
    "    # full_added = '_full'\n",
    "    try:\n",
    "        new_m.to_stata(\"Inputs/Created/merged\" + full_added + test_added + \".dta\", convert_dates={'date': 'tc', 'datadate': 'tc', 'rdq': 'tc'})\n",
    "    except:\n",
    "        new_m.to_stata(\"../Inputs/Created/merged\" + full_added + test_added + \".dta\", convert_dates={'date': 'tc', 'datadate': 'tc', 'rdq': 'tc'})\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
